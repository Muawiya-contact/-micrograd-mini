# ğŸ§  Micrograd-Mini

**Micrograd-Mini** is a tiny scalar-valued autograd engine with a basic neural network framework â€” built from scratch, inspired by [Andrej Karpathy's micrograd](https://github.com/karpathy/micrograd).

It supports reverse-mode automatic differentiation (backpropagation) and is capable of training a simple neural network (MLP) on tasks like XOR. Great for learning the internals of backprop!

---

## ğŸ”¥ Features

- âœ… Reverse-mode autodiff (backpropagation)
- âœ… Dynamic computation graph (DAG)
- âœ… Custom `Value` class with gradients
- âœ… Tanh and ReLU activations
- âœ… Basic Neuron, Layer, and MLP implementation
- âœ… Trains on XOR dataset

---

## ğŸ“ Project Structure

```
micrograd-mini/
    â”œâ”€â”€ engine.py # Core autodiff engine (Value class)
    â”œâ”€â”€ nn.py # Neural network components
    â”œâ”€â”€ train.py # Training loop for XOR
    â”œâ”€â”€ example.py # Better example using XOR dataset 
â””â”€â”€ README.md # Project info
```

---

## ğŸš€ Getting Started

### ğŸ”§ Requirements

- Python 3.7+
- No external libraries needed (pure Python)

### â–¶ï¸ Run Training

```
python train.py
```
# ğŸ“ˆ Example Output
```
--- Final Predictions after Training ---

Input: [0.0, 0.0] => Predicted: 0.01 | Target: 0.0
Input: [0.0, 1.0] => Predicted: 0.98 | Target: 1.0
Input: [1.0, 0.0] => Predicted: 0.97 | Target: 1.0
Input: [1.0, 1.0] => Predicted: 0.03 | Target: 0.0

Training complete! ğŸ¯
```
# ğŸ§  Learn by Building
Want to really understand backpropagation and gradients?

+ Dive into engine.py and explore the Value class

+ Inspect how operations dynamically build a graph

+ See how .backward() traverses it for gradient computation â€” just like real frameworks!
## ğŸ™ Attribution

This project is heavily inspired by [micrograd](https://github.com/karpathy/micrograd) by Andrej Karpathy, licensed under the MIT License.

---

## ğŸªª License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## âœ¨ Author

Built with â¤ï¸ by **Muawiya**, as part of a deep dive into AI, neural nets, and autodiff fundamentals.

### ğŸŒ Connect With Me

- ğŸ“º YouTube: [@Coding_Moves](https://www.youtube.com/@Coding_Moves)
- ğŸ’» GitHub: [Muawiya](https://github.com/Muawiya-contact) 
- ğŸ§  LeetCode: [Moavia_Amir](https://leetcode.com/u/Moavia_Amir/) 
- ğŸ“Š Kaggle: [Moavia Amir](https://www.kaggle.com/) 

---
